{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QzsgIb0A5jjI"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "data_dir = '/content/drive/My Drive/cs461/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M1L6j5oh0RKT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlvqxVPU6Kh1"
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "  def __init__(self):\n",
    "    self.use_cuda = True\n",
    "    self.log_interval = 20\n",
    "    self.train_batch_size = 64\n",
    "    self.test_batch_size = 64\n",
    "    self.lr = 0.01\n",
    "    self.momentum = 0.9\n",
    "    self.num_epochs = 3\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "C5C2cdDp_qdY",
    "outputId": "389b4a26-63c4-463c-948d-d52ca24e623e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "if args.use_cuda and torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "\n",
    "print('Using {}.'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOiqQ3Az1Nrp"
   },
   "source": [
    "Load dataset and create data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i08eXE7k1Bsu"
   },
   "outputs": [],
   "source": [
    "def prepare_data(args):\n",
    "  kwargs = {}\n",
    "  if args.use_cuda and torch.cuda.is_available():\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "  transform = transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.5,), (0.5,)),\n",
    "  ])\n",
    "\n",
    "  # Load original training data.\n",
    "  trainval_set = torchvision.datasets.FashionMNIST(\n",
    "      root=data_dir, train=True, download=True, transform=transform)\n",
    "\n",
    "  # Split original training data into training set and validation set\n",
    "  # and create data loaders.\n",
    "  train_set = torch.utils.data.Subset(trainval_set, range(50000))\n",
    "  train_loader = torch.utils.data.DataLoader(\n",
    "      train_set, batch_size=args.train_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "  val_set = torch.utils.data.Subset(trainval_set, range(50000, 60000))\n",
    "  val_loader = torch.utils.data.DataLoader(\n",
    "      val_set, batch_size=args.test_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "  # Load testing data and create data loader.\n",
    "  test_set = torchvision.datasets.FashionMNIST(\n",
    "      root=data_dir, train=False, download=True, transform=transform)\n",
    "  test_loader = torch.utils.data.DataLoader(\n",
    "      test_set, batch_size=args.test_batch_size, shuffle=False, **kwargs)\n",
    "  \n",
    "  return train_set, val_set, test_set, train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8kiyHpkU-ugv"
   },
   "source": [
    "Visualize some training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "colab_type": "code",
    "id": "ab1ssgr47n4p",
    "outputId": "66052159-08ca-442b-9974-314731facf1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /content/drive/My Drive/cs461/data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26427392it [00:04, 5806910.29it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /content/drive/My Drive/cs461/data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /content/drive/My Drive/cs461/data/FashionMNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /content/drive/My Drive/cs461/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 41275.66it/s]                           \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /content/drive/My Drive/cs461/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /content/drive/My Drive/cs461/data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /content/drive/My Drive/cs461/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4423680it [00:02, 1661211.92it/s]                            \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /content/drive/My Drive/cs461/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /content/drive/My Drive/cs461/data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /content/drive/My Drive/cs461/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 13927.94it/s]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /content/drive/My Drive/cs461/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /content/drive/My Drive/cs461/data/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAA+CAYAAAAVksF/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nomClsnUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set, val_set, test_set, train_loader, val_loader, test_loader = prepare_data(args)\n",
    "\n",
    "fig, axes = plt.subplots(1, 10)\n",
    "for idx, ax in enumerate(axes):\n",
    "  img, lbl = train_set[idx]\n",
    "  ax.imshow(img[0], cmap='gray')\n",
    "  ax.set_title(lbl)\n",
    "  ax.set_xticklabels([])\n",
    "  ax.set_yticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RvcEKGCT-3Hg"
   },
   "source": [
    "Define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Na6u8dr5-2ww"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "    self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    self._initialize_weights()\n",
    "  \n",
    "  def _initialize_weights(self):\n",
    "    pass\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.conv1(x))\n",
    "    x = F.max_pool2d(x, 2, stride=2)\n",
    "    x = F.relu(self.conv2(x))\n",
    "    x = F.max_pool2d(x, 2, stride=2)\n",
    "    x = torch.flatten(x, start_dim=1)\n",
    "    x = self.fc3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-DZrXbkC_JBA"
   },
   "source": [
    "Define the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RYOzxAhw_Ida"
   },
   "outputs": [],
   "source": [
    "def train(args, model, criterion, train_loader, optimizer, device):\n",
    "  model.train()\n",
    "  total_loss = 0.\n",
    "  for i, data in enumerate(train_loader):\n",
    "    imgs, lbls = data[0].to(device), data[1].to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(imgs)\n",
    "    loss = criterion(outputs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    if (i + 1) % args.log_interval == 0:\n",
    "      mean_loss = total_loss / args.log_interval\n",
    "      print('  batch {:4d}: loss={:.3f}'.format(i + 1, mean_loss))\n",
    "      total_loss = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fCcrktfM_PDE"
   },
   "source": [
    "Define the testing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zay7qwD8_RWy"
   },
   "outputs": [],
   "source": [
    "def test(args, model, test_loader, device):\n",
    "  model.eval()\n",
    "  total, correct = 0, 0\n",
    "  with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "      imgs, lbls = data[0].to(device), data[1].to(device)\n",
    "      outputs = model(imgs)\n",
    "      _, preds = torch.max(outputs.data, 1)\n",
    "      total += lbls.shape[0]\n",
    "      correct += (preds == lbls).sum().item()\n",
    "\n",
    "  acc = correct / total\n",
    "  print('  acc={:.3f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CmK6qQxB_VAQ"
   },
   "source": [
    "Run training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dUSfV5IB_UZ-",
    "outputId": "ed60515a-729b-49ce-c10c-dcdc480c9dd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "  batch   20: loss=2.220\n",
      "  batch   40: loss=1.519\n",
      "  batch   60: loss=0.880\n",
      "  batch   80: loss=0.774\n",
      "  batch  100: loss=0.773\n",
      "  batch  120: loss=0.721\n",
      "  batch  140: loss=0.658\n",
      "  batch  160: loss=0.667\n",
      "  batch  180: loss=0.613\n",
      "  batch  200: loss=0.633\n",
      "  batch  220: loss=0.609\n",
      "  batch  240: loss=0.612\n",
      "  batch  260: loss=0.609\n",
      "  batch  280: loss=0.585\n",
      "  batch  300: loss=0.533\n",
      "  batch  320: loss=0.515\n",
      "  batch  340: loss=0.611\n",
      "  batch  360: loss=0.556\n",
      "  batch  380: loss=0.553\n",
      "  batch  400: loss=0.486\n",
      "  batch  420: loss=0.490\n",
      "  batch  440: loss=0.482\n",
      "  batch  460: loss=0.514\n",
      "  batch  480: loss=0.469\n",
      "  batch  500: loss=0.474\n",
      "  batch  520: loss=0.483\n",
      "  batch  540: loss=0.451\n",
      "  batch  560: loss=0.490\n",
      "  batch  580: loss=0.500\n",
      "  batch  600: loss=0.433\n",
      "  batch  620: loss=0.515\n",
      "  batch  640: loss=0.449\n",
      "  batch  660: loss=0.473\n",
      "  batch  680: loss=0.418\n",
      "  batch  700: loss=0.464\n",
      "  batch  720: loss=0.440\n",
      "  batch  740: loss=0.415\n",
      "  batch  760: loss=0.436\n",
      "  batch  780: loss=0.409\n",
      "Testing on validation set\n",
      "  acc=0.846\n",
      "Training epoch 1\n",
      "  batch   20: loss=0.360\n",
      "  batch   40: loss=0.464\n",
      "  batch   60: loss=0.448\n",
      "  batch   80: loss=0.405\n",
      "  batch  100: loss=0.431\n",
      "  batch  120: loss=0.416\n",
      "  batch  140: loss=0.384\n",
      "  batch  160: loss=0.400\n",
      "  batch  180: loss=0.404\n",
      "  batch  200: loss=0.365\n",
      "  batch  220: loss=0.425\n",
      "  batch  240: loss=0.397\n",
      "  batch  260: loss=0.412\n",
      "  batch  280: loss=0.447\n",
      "  batch  300: loss=0.425\n",
      "  batch  320: loss=0.401\n",
      "  batch  340: loss=0.388\n",
      "  batch  360: loss=0.404\n",
      "  batch  380: loss=0.405\n",
      "  batch  400: loss=0.419\n",
      "  batch  420: loss=0.356\n",
      "  batch  440: loss=0.393\n",
      "  batch  460: loss=0.390\n",
      "  batch  480: loss=0.376\n",
      "  batch  500: loss=0.349\n",
      "  batch  520: loss=0.381\n",
      "  batch  540: loss=0.365\n",
      "  batch  560: loss=0.391\n",
      "  batch  580: loss=0.404\n",
      "  batch  600: loss=0.379\n",
      "  batch  620: loss=0.365\n",
      "  batch  640: loss=0.399\n",
      "  batch  660: loss=0.362\n",
      "  batch  680: loss=0.401\n",
      "  batch  700: loss=0.416\n",
      "  batch  720: loss=0.409\n",
      "  batch  740: loss=0.366\n",
      "  batch  760: loss=0.379\n",
      "  batch  780: loss=0.360\n",
      "Testing on validation set\n",
      "  acc=0.863\n",
      "Training epoch 2\n",
      "  batch   20: loss=0.383\n",
      "  batch   40: loss=0.352\n",
      "  batch   60: loss=0.378\n",
      "  batch   80: loss=0.353\n",
      "  batch  100: loss=0.333\n",
      "  batch  120: loss=0.351\n",
      "  batch  140: loss=0.364\n",
      "  batch  160: loss=0.388\n",
      "  batch  180: loss=0.364\n",
      "  batch  200: loss=0.347\n",
      "  batch  220: loss=0.393\n",
      "  batch  240: loss=0.342\n",
      "  batch  260: loss=0.340\n",
      "  batch  280: loss=0.348\n",
      "  batch  300: loss=0.314\n",
      "  batch  320: loss=0.378\n",
      "  batch  340: loss=0.367\n",
      "  batch  360: loss=0.380\n",
      "  batch  380: loss=0.332\n",
      "  batch  400: loss=0.368\n",
      "  batch  420: loss=0.337\n",
      "  batch  440: loss=0.342\n",
      "  batch  460: loss=0.351\n",
      "  batch  480: loss=0.342\n",
      "  batch  500: loss=0.316\n",
      "  batch  520: loss=0.374\n",
      "  batch  540: loss=0.320\n",
      "  batch  560: loss=0.356\n",
      "  batch  580: loss=0.325\n",
      "  batch  600: loss=0.334\n",
      "  batch  620: loss=0.362\n",
      "  batch  640: loss=0.410\n",
      "  batch  660: loss=0.342\n",
      "  batch  680: loss=0.372\n",
      "  batch  700: loss=0.363\n",
      "  batch  720: loss=0.328\n",
      "  batch  740: loss=0.340\n",
      "  batch  760: loss=0.328\n",
      "  batch  780: loss=0.324\n",
      "Testing on validation set\n",
      "  acc=0.875\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=args.lr,\n",
    "    momentum=args.momentum\n",
    ")\n",
    "\n",
    "for e in range(args.num_epochs):\n",
    "  print('Training epoch {}'.format(e))\n",
    "  train(args, model, criterion, train_loader, optimizer, device)\n",
    "  print('Testing on validation set')\n",
    "  test(args, model, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YZPW0Mx4cMEd"
   },
   "source": [
    "Question 1: Evaluating trained model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8OgiAGMscKIr",
    "outputId": "28ac0c83-227a-4f6c-8d63-d3c17383011b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  acc=0.870\n"
     ]
    }
   ],
   "source": [
    "test(args, model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8J7Xm_uf0Lra"
   },
   "source": [
    "Question 2: Modifying model & training scheme for improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-zPJ5_f0mWd"
   },
   "source": [
    "Part 1: Smaller batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pl-a2UNQ0UVk",
    "outputId": "7593b54c-30a5-4b58-babb-311cec208b24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "  batch   20: loss=2.214\n",
      "  batch   40: loss=1.488\n",
      "  batch   60: loss=1.027\n",
      "  batch   80: loss=0.858\n",
      "  batch  100: loss=0.797\n",
      "  batch  120: loss=0.744\n",
      "  batch  140: loss=0.680\n",
      "  batch  160: loss=0.659\n",
      "  batch  180: loss=0.640\n",
      "  batch  200: loss=0.660\n",
      "  batch  220: loss=0.611\n",
      "  batch  240: loss=0.599\n",
      "  batch  260: loss=0.603\n",
      "  batch  280: loss=0.582\n",
      "  batch  300: loss=0.547\n",
      "  batch  320: loss=0.566\n",
      "  batch  340: loss=0.545\n",
      "  batch  360: loss=0.566\n",
      "  batch  380: loss=0.530\n",
      "  batch  400: loss=0.518\n",
      "  batch  420: loss=0.526\n",
      "  batch  440: loss=0.506\n",
      "  batch  460: loss=0.464\n",
      "  batch  480: loss=0.469\n",
      "  batch  500: loss=0.466\n",
      "  batch  520: loss=0.435\n",
      "  batch  540: loss=0.472\n",
      "  batch  560: loss=0.494\n",
      "  batch  580: loss=0.443\n",
      "  batch  600: loss=0.510\n",
      "  batch  620: loss=0.434\n",
      "  batch  640: loss=0.441\n",
      "  batch  660: loss=0.450\n",
      "  batch  680: loss=0.432\n",
      "  batch  700: loss=0.474\n",
      "  batch  720: loss=0.451\n",
      "  batch  740: loss=0.411\n",
      "  batch  760: loss=0.421\n",
      "  batch  780: loss=0.407\n",
      "Testing on validation set\n",
      "  acc=0.844\n",
      "Training epoch 1\n",
      "  batch   20: loss=0.405\n",
      "  batch   40: loss=0.410\n",
      "  batch   60: loss=0.428\n",
      "  batch   80: loss=0.436\n",
      "  batch  100: loss=0.414\n",
      "  batch  120: loss=0.385\n",
      "  batch  140: loss=0.405\n",
      "  batch  160: loss=0.424\n",
      "  batch  180: loss=0.415\n",
      "  batch  200: loss=0.398\n",
      "  batch  220: loss=0.384\n",
      "  batch  240: loss=0.448\n",
      "  batch  260: loss=0.388\n",
      "  batch  280: loss=0.423\n",
      "  batch  300: loss=0.364\n",
      "  batch  320: loss=0.409\n",
      "  batch  340: loss=0.390\n",
      "  batch  360: loss=0.417\n",
      "  batch  380: loss=0.377\n",
      "  batch  400: loss=0.411\n",
      "  batch  420: loss=0.417\n",
      "  batch  440: loss=0.431\n",
      "  batch  460: loss=0.439\n",
      "  batch  480: loss=0.408\n",
      "  batch  500: loss=0.377\n",
      "  batch  520: loss=0.366\n",
      "  batch  540: loss=0.388\n",
      "  batch  560: loss=0.425\n",
      "  batch  580: loss=0.351\n",
      "  batch  600: loss=0.414\n",
      "  batch  620: loss=0.375\n",
      "  batch  640: loss=0.402\n",
      "  batch  660: loss=0.379\n",
      "  batch  680: loss=0.345\n",
      "  batch  700: loss=0.411\n",
      "  batch  720: loss=0.359\n",
      "  batch  740: loss=0.403\n",
      "  batch  760: loss=0.326\n",
      "  batch  780: loss=0.368\n",
      "Testing on validation set\n",
      "  acc=0.865\n",
      "Training epoch 2\n",
      "  batch   20: loss=0.385\n",
      "  batch   40: loss=0.353\n",
      "  batch   60: loss=0.401\n",
      "  batch   80: loss=0.320\n",
      "  batch  100: loss=0.378\n",
      "  batch  120: loss=0.328\n",
      "  batch  140: loss=0.357\n",
      "  batch  160: loss=0.348\n",
      "  batch  180: loss=0.359\n",
      "  batch  200: loss=0.370\n",
      "  batch  220: loss=0.376\n",
      "  batch  240: loss=0.340\n",
      "  batch  260: loss=0.326\n",
      "  batch  280: loss=0.348\n",
      "  batch  300: loss=0.345\n",
      "  batch  320: loss=0.363\n",
      "  batch  340: loss=0.344\n",
      "  batch  360: loss=0.343\n",
      "  batch  380: loss=0.334\n",
      "  batch  400: loss=0.365\n",
      "  batch  420: loss=0.351\n",
      "  batch  440: loss=0.382\n",
      "  batch  460: loss=0.377\n",
      "  batch  480: loss=0.356\n",
      "  batch  500: loss=0.348\n",
      "  batch  520: loss=0.383\n",
      "  batch  540: loss=0.345\n",
      "  batch  560: loss=0.344\n",
      "  batch  580: loss=0.314\n",
      "  batch  600: loss=0.354\n",
      "  batch  620: loss=0.319\n",
      "  batch  640: loss=0.375\n",
      "  batch  660: loss=0.362\n",
      "  batch  680: loss=0.349\n",
      "  batch  700: loss=0.339\n",
      "  batch  720: loss=0.384\n",
      "  batch  740: loss=0.366\n",
      "  batch  760: loss=0.315\n",
      "  batch  780: loss=0.379\n",
      "Testing on validation set\n",
      "  acc=0.874\n"
     ]
    }
   ],
   "source": [
    "model1 = Net()\n",
    "model1.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model1.parameters(),\n",
    "    lr=args.lr,\n",
    "    momentum=args.momentum\n",
    ")\n",
    "\n",
    "args.train_batch_size = 32\n",
    "\n",
    "for e in range(args.num_epochs):\n",
    "  print('Training epoch {}'.format(e))\n",
    "  train(args, model1, criterion, train_loader, optimizer, device)\n",
    "  print('Testing on validation set')\n",
    "  test(args, model1, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UuygJBIM05tW"
   },
   "source": [
    "A smaller batch size of 32 decreased accuracy very slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vk9l2XQy1Axp"
   },
   "source": [
    "Part 2: More epochs (original batch size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "WCkVtEei1DNT",
    "outputId": "b5b9fdb0-d6ea-430a-a74c-41f462500736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "  batch   20: loss=2.184\n",
      "  batch   40: loss=1.409\n",
      "  batch   60: loss=0.972\n",
      "  batch   80: loss=0.875\n",
      "  batch  100: loss=0.827\n",
      "  batch  120: loss=0.731\n",
      "  batch  140: loss=0.642\n",
      "  batch  160: loss=0.642\n",
      "  batch  180: loss=0.583\n",
      "  batch  200: loss=0.618\n",
      "  batch  220: loss=0.651\n",
      "  batch  240: loss=0.583\n",
      "  batch  260: loss=0.569\n",
      "  batch  280: loss=0.580\n",
      "  batch  300: loss=0.532\n",
      "  batch  320: loss=0.526\n",
      "  batch  340: loss=0.554\n",
      "  batch  360: loss=0.480\n",
      "  batch  380: loss=0.477\n",
      "  batch  400: loss=0.489\n",
      "  batch  420: loss=0.497\n",
      "  batch  440: loss=0.534\n",
      "  batch  460: loss=0.513\n",
      "  batch  480: loss=0.504\n",
      "  batch  500: loss=0.437\n",
      "  batch  520: loss=0.472\n",
      "  batch  540: loss=0.463\n",
      "  batch  560: loss=0.448\n",
      "  batch  580: loss=0.464\n",
      "  batch  600: loss=0.491\n",
      "  batch  620: loss=0.437\n",
      "  batch  640: loss=0.414\n",
      "  batch  660: loss=0.467\n",
      "  batch  680: loss=0.477\n",
      "  batch  700: loss=0.449\n",
      "  batch  720: loss=0.427\n",
      "  batch  740: loss=0.406\n",
      "  batch  760: loss=0.410\n",
      "  batch  780: loss=0.426\n",
      "Testing on validation set\n",
      "  acc=0.841\n",
      "Training epoch 1\n",
      "  batch   20: loss=0.432\n",
      "  batch   40: loss=0.447\n",
      "  batch   60: loss=0.412\n",
      "  batch   80: loss=0.365\n",
      "  batch  100: loss=0.425\n",
      "  batch  120: loss=0.425\n",
      "  batch  140: loss=0.425\n",
      "  batch  160: loss=0.382\n",
      "  batch  180: loss=0.417\n",
      "  batch  200: loss=0.398\n",
      "  batch  220: loss=0.378\n",
      "  batch  240: loss=0.408\n",
      "  batch  260: loss=0.390\n",
      "  batch  280: loss=0.396\n",
      "  batch  300: loss=0.406\n",
      "  batch  320: loss=0.431\n",
      "  batch  340: loss=0.414\n",
      "  batch  360: loss=0.387\n",
      "  batch  380: loss=0.350\n",
      "  batch  400: loss=0.373\n",
      "  batch  420: loss=0.411\n",
      "  batch  440: loss=0.394\n",
      "  batch  460: loss=0.378\n",
      "  batch  480: loss=0.360\n",
      "  batch  500: loss=0.385\n",
      "  batch  520: loss=0.380\n",
      "  batch  540: loss=0.368\n",
      "  batch  560: loss=0.420\n",
      "  batch  580: loss=0.353\n",
      "  batch  600: loss=0.357\n",
      "  batch  620: loss=0.394\n",
      "  batch  640: loss=0.387\n",
      "  batch  660: loss=0.392\n",
      "  batch  680: loss=0.361\n",
      "  batch  700: loss=0.393\n",
      "  batch  720: loss=0.379\n",
      "  batch  740: loss=0.368\n",
      "  batch  760: loss=0.393\n",
      "  batch  780: loss=0.354\n",
      "Testing on validation set\n",
      "  acc=0.854\n",
      "Training epoch 2\n",
      "  batch   20: loss=0.359\n",
      "  batch   40: loss=0.350\n",
      "  batch   60: loss=0.347\n",
      "  batch   80: loss=0.351\n",
      "  batch  100: loss=0.366\n",
      "  batch  120: loss=0.380\n",
      "  batch  140: loss=0.379\n",
      "  batch  160: loss=0.344\n",
      "  batch  180: loss=0.352\n",
      "  batch  200: loss=0.370\n",
      "  batch  220: loss=0.343\n",
      "  batch  240: loss=0.371\n",
      "  batch  260: loss=0.345\n",
      "  batch  280: loss=0.342\n",
      "  batch  300: loss=0.365\n",
      "  batch  320: loss=0.388\n",
      "  batch  340: loss=0.312\n",
      "  batch  360: loss=0.352\n",
      "  batch  380: loss=0.341\n",
      "  batch  400: loss=0.351\n",
      "  batch  420: loss=0.367\n",
      "  batch  440: loss=0.364\n",
      "  batch  460: loss=0.361\n",
      "  batch  480: loss=0.323\n",
      "  batch  500: loss=0.360\n",
      "  batch  520: loss=0.353\n",
      "  batch  540: loss=0.363\n",
      "  batch  560: loss=0.347\n",
      "  batch  580: loss=0.331\n",
      "  batch  600: loss=0.306\n",
      "  batch  620: loss=0.349\n",
      "  batch  640: loss=0.340\n",
      "  batch  660: loss=0.352\n",
      "  batch  680: loss=0.320\n",
      "  batch  700: loss=0.344\n",
      "  batch  720: loss=0.361\n",
      "  batch  740: loss=0.307\n",
      "  batch  760: loss=0.388\n",
      "  batch  780: loss=0.357\n",
      "Testing on validation set\n",
      "  acc=0.870\n",
      "Training epoch 3\n",
      "  batch   20: loss=0.330\n",
      "  batch   40: loss=0.346\n",
      "  batch   60: loss=0.306\n",
      "  batch   80: loss=0.325\n",
      "  batch  100: loss=0.302\n",
      "  batch  120: loss=0.308\n",
      "  batch  140: loss=0.354\n",
      "  batch  160: loss=0.318\n",
      "  batch  180: loss=0.307\n",
      "  batch  200: loss=0.325\n",
      "  batch  220: loss=0.318\n",
      "  batch  240: loss=0.337\n",
      "  batch  260: loss=0.338\n",
      "  batch  280: loss=0.323\n",
      "  batch  300: loss=0.292\n",
      "  batch  320: loss=0.344\n",
      "  batch  340: loss=0.335\n",
      "  batch  360: loss=0.332\n",
      "  batch  380: loss=0.345\n",
      "  batch  400: loss=0.325\n",
      "  batch  420: loss=0.349\n",
      "  batch  440: loss=0.323\n",
      "  batch  460: loss=0.334\n",
      "  batch  480: loss=0.345\n",
      "  batch  500: loss=0.328\n",
      "  batch  520: loss=0.318\n",
      "  batch  540: loss=0.298\n",
      "  batch  560: loss=0.306\n",
      "  batch  580: loss=0.368\n",
      "  batch  600: loss=0.347\n",
      "  batch  620: loss=0.339\n",
      "  batch  640: loss=0.313\n",
      "  batch  660: loss=0.352\n",
      "  batch  680: loss=0.331\n",
      "  batch  700: loss=0.286\n",
      "  batch  720: loss=0.367\n",
      "  batch  740: loss=0.311\n",
      "  batch  760: loss=0.307\n",
      "  batch  780: loss=0.338\n",
      "Testing on validation set\n",
      "  acc=0.873\n",
      "Training epoch 4\n",
      "  batch   20: loss=0.307\n",
      "  batch   40: loss=0.306\n",
      "  batch   60: loss=0.315\n",
      "  batch   80: loss=0.340\n",
      "  batch  100: loss=0.312\n",
      "  batch  120: loss=0.308\n",
      "  batch  140: loss=0.281\n",
      "  batch  160: loss=0.284\n",
      "  batch  180: loss=0.329\n",
      "  batch  200: loss=0.315\n",
      "  batch  220: loss=0.321\n",
      "  batch  240: loss=0.294\n",
      "  batch  260: loss=0.291\n",
      "  batch  280: loss=0.320\n",
      "  batch  300: loss=0.331\n",
      "  batch  320: loss=0.320\n",
      "  batch  340: loss=0.320\n",
      "  batch  360: loss=0.296\n",
      "  batch  380: loss=0.288\n",
      "  batch  400: loss=0.336\n",
      "  batch  420: loss=0.327\n",
      "  batch  440: loss=0.325\n",
      "  batch  460: loss=0.340\n",
      "  batch  480: loss=0.325\n",
      "  batch  500: loss=0.294\n",
      "  batch  520: loss=0.292\n",
      "  batch  540: loss=0.334\n",
      "  batch  560: loss=0.335\n",
      "  batch  580: loss=0.297\n",
      "  batch  600: loss=0.313\n",
      "  batch  620: loss=0.325\n",
      "  batch  640: loss=0.323\n",
      "  batch  660: loss=0.327\n",
      "  batch  680: loss=0.275\n",
      "  batch  700: loss=0.292\n",
      "  batch  720: loss=0.326\n",
      "  batch  740: loss=0.335\n",
      "  batch  760: loss=0.293\n",
      "  batch  780: loss=0.284\n",
      "Testing on validation set\n",
      "  acc=0.875\n",
      "Training epoch 5\n",
      "  batch   20: loss=0.318\n",
      "  batch   40: loss=0.334\n",
      "  batch   60: loss=0.308\n",
      "  batch   80: loss=0.288\n",
      "  batch  100: loss=0.300\n",
      "  batch  120: loss=0.310\n",
      "  batch  140: loss=0.289\n",
      "  batch  160: loss=0.291\n",
      "  batch  180: loss=0.289\n",
      "  batch  200: loss=0.321\n",
      "  batch  220: loss=0.307\n",
      "  batch  240: loss=0.285\n",
      "  batch  260: loss=0.313\n",
      "  batch  280: loss=0.313\n",
      "  batch  300: loss=0.320\n",
      "  batch  320: loss=0.263\n",
      "  batch  340: loss=0.266\n",
      "  batch  360: loss=0.329\n",
      "  batch  380: loss=0.316\n",
      "  batch  400: loss=0.308\n",
      "  batch  420: loss=0.295\n",
      "  batch  440: loss=0.324\n",
      "  batch  460: loss=0.322\n",
      "  batch  480: loss=0.281\n",
      "  batch  500: loss=0.303\n",
      "  batch  520: loss=0.302\n",
      "  batch  540: loss=0.334\n",
      "  batch  560: loss=0.281\n",
      "  batch  580: loss=0.291\n",
      "  batch  600: loss=0.276\n",
      "  batch  620: loss=0.272\n",
      "  batch  640: loss=0.300\n",
      "  batch  660: loss=0.319\n",
      "  batch  680: loss=0.300\n",
      "  batch  700: loss=0.305\n",
      "  batch  720: loss=0.289\n",
      "  batch  740: loss=0.310\n",
      "  batch  760: loss=0.295\n",
      "  batch  780: loss=0.282\n",
      "Testing on validation set\n",
      "  acc=0.876\n",
      "Training epoch 6\n",
      "  batch   20: loss=0.301\n",
      "  batch   40: loss=0.294\n",
      "  batch   60: loss=0.265\n",
      "  batch   80: loss=0.282\n",
      "  batch  100: loss=0.296\n",
      "  batch  120: loss=0.314\n",
      "  batch  140: loss=0.329\n",
      "  batch  160: loss=0.276\n",
      "  batch  180: loss=0.268\n",
      "  batch  200: loss=0.312\n",
      "  batch  220: loss=0.306\n",
      "  batch  240: loss=0.281\n",
      "  batch  260: loss=0.325\n",
      "  batch  280: loss=0.289\n",
      "  batch  300: loss=0.331\n",
      "  batch  320: loss=0.283\n",
      "  batch  340: loss=0.334\n",
      "  batch  360: loss=0.302\n",
      "  batch  380: loss=0.282\n",
      "  batch  400: loss=0.268\n",
      "  batch  420: loss=0.311\n",
      "  batch  440: loss=0.308\n",
      "  batch  460: loss=0.281\n",
      "  batch  480: loss=0.287\n",
      "  batch  500: loss=0.297\n",
      "  batch  520: loss=0.316\n",
      "  batch  540: loss=0.246\n",
      "  batch  560: loss=0.287\n",
      "  batch  580: loss=0.296\n",
      "  batch  600: loss=0.221\n",
      "  batch  620: loss=0.282\n",
      "  batch  640: loss=0.259\n",
      "  batch  660: loss=0.316\n",
      "  batch  680: loss=0.289\n",
      "  batch  700: loss=0.287\n",
      "  batch  720: loss=0.320\n",
      "  batch  740: loss=0.313\n",
      "  batch  760: loss=0.307\n",
      "  batch  780: loss=0.311\n",
      "Testing on validation set\n",
      "  acc=0.871\n",
      "Training epoch 7\n",
      "  batch   20: loss=0.288\n",
      "  batch   40: loss=0.280\n",
      "  batch   60: loss=0.255\n",
      "  batch   80: loss=0.294\n",
      "  batch  100: loss=0.276\n",
      "  batch  120: loss=0.262\n",
      "  batch  140: loss=0.282\n",
      "  batch  160: loss=0.305\n",
      "  batch  180: loss=0.276\n",
      "  batch  200: loss=0.309\n",
      "  batch  220: loss=0.308\n",
      "  batch  240: loss=0.272\n",
      "  batch  260: loss=0.326\n",
      "  batch  280: loss=0.291\n",
      "  batch  300: loss=0.301\n",
      "  batch  320: loss=0.294\n",
      "  batch  340: loss=0.265\n",
      "  batch  360: loss=0.261\n",
      "  batch  380: loss=0.301\n",
      "  batch  400: loss=0.297\n",
      "  batch  420: loss=0.283\n",
      "  batch  440: loss=0.332\n",
      "  batch  460: loss=0.258\n",
      "  batch  480: loss=0.319\n",
      "  batch  500: loss=0.279\n",
      "  batch  520: loss=0.273\n",
      "  batch  540: loss=0.286\n",
      "  batch  560: loss=0.267\n",
      "  batch  580: loss=0.277\n",
      "  batch  600: loss=0.278\n",
      "  batch  620: loss=0.259\n",
      "  batch  640: loss=0.270\n",
      "  batch  660: loss=0.321\n",
      "  batch  680: loss=0.282\n",
      "  batch  700: loss=0.296\n",
      "  batch  720: loss=0.283\n",
      "  batch  740: loss=0.290\n",
      "  batch  760: loss=0.282\n",
      "  batch  780: loss=0.289\n",
      "Testing on validation set\n",
      "  acc=0.885\n",
      "Training epoch 8\n",
      "  batch   20: loss=0.276\n",
      "  batch   40: loss=0.282\n",
      "  batch   60: loss=0.271\n",
      "  batch   80: loss=0.261\n",
      "  batch  100: loss=0.247\n",
      "  batch  120: loss=0.295\n",
      "  batch  140: loss=0.275\n",
      "  batch  160: loss=0.270\n",
      "  batch  180: loss=0.285\n",
      "  batch  200: loss=0.263\n",
      "  batch  220: loss=0.313\n",
      "  batch  240: loss=0.275\n",
      "  batch  260: loss=0.274\n",
      "  batch  280: loss=0.273\n",
      "  batch  300: loss=0.314\n",
      "  batch  320: loss=0.297\n",
      "  batch  340: loss=0.276\n",
      "  batch  360: loss=0.267\n",
      "  batch  380: loss=0.258\n",
      "  batch  400: loss=0.271\n",
      "  batch  420: loss=0.304\n",
      "  batch  440: loss=0.276\n",
      "  batch  460: loss=0.287\n",
      "  batch  480: loss=0.301\n",
      "  batch  500: loss=0.271\n",
      "  batch  520: loss=0.266\n",
      "  batch  540: loss=0.283\n",
      "  batch  560: loss=0.307\n",
      "  batch  580: loss=0.280\n",
      "  batch  600: loss=0.261\n",
      "  batch  620: loss=0.248\n",
      "  batch  640: loss=0.266\n",
      "  batch  660: loss=0.267\n",
      "  batch  680: loss=0.294\n",
      "  batch  700: loss=0.317\n",
      "  batch  720: loss=0.317\n",
      "  batch  740: loss=0.268\n",
      "  batch  760: loss=0.283\n",
      "  batch  780: loss=0.279\n",
      "Testing on validation set\n",
      "  acc=0.887\n",
      "Training epoch 9\n",
      "  batch   20: loss=0.271\n",
      "  batch   40: loss=0.274\n",
      "  batch   60: loss=0.270\n",
      "  batch   80: loss=0.261\n",
      "  batch  100: loss=0.285\n",
      "  batch  120: loss=0.258\n",
      "  batch  140: loss=0.288\n",
      "  batch  160: loss=0.279\n",
      "  batch  180: loss=0.269\n",
      "  batch  200: loss=0.272\n",
      "  batch  220: loss=0.269\n",
      "  batch  240: loss=0.292\n",
      "  batch  260: loss=0.244\n",
      "  batch  280: loss=0.255\n",
      "  batch  300: loss=0.295\n",
      "  batch  320: loss=0.275\n",
      "  batch  340: loss=0.293\n",
      "  batch  360: loss=0.264\n",
      "  batch  380: loss=0.268\n",
      "  batch  400: loss=0.282\n",
      "  batch  420: loss=0.270\n",
      "  batch  440: loss=0.269\n",
      "  batch  460: loss=0.284\n",
      "  batch  480: loss=0.248\n",
      "  batch  500: loss=0.255\n",
      "  batch  520: loss=0.308\n",
      "  batch  540: loss=0.249\n",
      "  batch  560: loss=0.256\n",
      "  batch  580: loss=0.327\n",
      "  batch  600: loss=0.266\n",
      "  batch  620: loss=0.255\n",
      "  batch  640: loss=0.295\n",
      "  batch  660: loss=0.272\n",
      "  batch  680: loss=0.274\n",
      "  batch  700: loss=0.280\n",
      "  batch  720: loss=0.286\n",
      "  batch  740: loss=0.285\n",
      "  batch  760: loss=0.292\n",
      "  batch  780: loss=0.288\n",
      "Testing on validation set\n",
      "  acc=0.883\n"
     ]
    }
   ],
   "source": [
    "model2 = Net()\n",
    "model2.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model2.parameters(),\n",
    "    lr=args.lr,\n",
    "    momentum=args.momentum\n",
    ")\n",
    "\n",
    "args.num_epochs = 10\n",
    "args.train_batch_size = 64\n",
    "\n",
    "for e in range(args.num_epochs):\n",
    "  print('Training epoch {}'.format(e))\n",
    "  train(args, model2, criterion, train_loader, optimizer, device)\n",
    "  print('Testing on validation set')\n",
    "  test(args, model2, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gjtqMkF82re9"
   },
   "source": [
    "Increasing the number of epochs increased accuracy by over 1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U67m62Pq29rL"
   },
   "source": [
    "Part 3: Lower learning rate (increased number of epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Xg95KNdt2urp",
    "outputId": "7279c0dc-c5fa-453f-a73c-34b3e5fe1510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "  batch   20: loss=2.296\n",
      "  batch   40: loss=2.268\n",
      "  batch   60: loss=2.220\n",
      "  batch   80: loss=2.173\n",
      "  batch  100: loss=2.078\n",
      "  batch  120: loss=1.946\n",
      "  batch  140: loss=1.767\n",
      "  batch  160: loss=1.592\n",
      "  batch  180: loss=1.390\n",
      "  batch  200: loss=1.207\n",
      "  batch  220: loss=1.086\n",
      "  batch  240: loss=1.018\n",
      "  batch  260: loss=0.966\n",
      "  batch  280: loss=0.947\n",
      "  batch  300: loss=0.884\n",
      "  batch  320: loss=0.873\n",
      "  batch  340: loss=0.865\n",
      "  batch  360: loss=0.787\n",
      "  batch  380: loss=0.813\n",
      "  batch  400: loss=0.798\n",
      "  batch  420: loss=0.828\n",
      "  batch  440: loss=0.728\n",
      "  batch  460: loss=0.762\n",
      "  batch  480: loss=0.757\n",
      "  batch  500: loss=0.772\n",
      "  batch  520: loss=0.761\n",
      "  batch  540: loss=0.743\n",
      "  batch  560: loss=0.668\n",
      "  batch  580: loss=0.709\n",
      "  batch  600: loss=0.706\n",
      "  batch  620: loss=0.715\n",
      "  batch  640: loss=0.670\n",
      "  batch  660: loss=0.713\n",
      "  batch  680: loss=0.719\n",
      "  batch  700: loss=0.668\n",
      "  batch  720: loss=0.696\n",
      "  batch  740: loss=0.646\n",
      "  batch  760: loss=0.681\n",
      "  batch  780: loss=0.660\n",
      "Testing on validation set\n",
      "  acc=0.759\n",
      "Training epoch 1\n",
      "  batch   20: loss=0.629\n",
      "  batch   40: loss=0.653\n",
      "  batch   60: loss=0.669\n",
      "  batch   80: loss=0.641\n",
      "  batch  100: loss=0.632\n",
      "  batch  120: loss=0.666\n",
      "  batch  140: loss=0.662\n",
      "  batch  160: loss=0.687\n",
      "  batch  180: loss=0.640\n",
      "  batch  200: loss=0.633\n",
      "  batch  220: loss=0.625\n",
      "  batch  240: loss=0.626\n",
      "  batch  260: loss=0.624\n",
      "  batch  280: loss=0.640\n",
      "  batch  300: loss=0.636\n",
      "  batch  320: loss=0.697\n",
      "  batch  340: loss=0.589\n",
      "  batch  360: loss=0.624\n",
      "  batch  380: loss=0.628\n",
      "  batch  400: loss=0.607\n",
      "  batch  420: loss=0.605\n",
      "  batch  440: loss=0.585\n",
      "  batch  460: loss=0.577\n",
      "  batch  480: loss=0.629\n",
      "  batch  500: loss=0.562\n",
      "  batch  520: loss=0.582\n",
      "  batch  540: loss=0.570\n",
      "  batch  560: loss=0.559\n",
      "  batch  580: loss=0.559\n",
      "  batch  600: loss=0.599\n",
      "  batch  620: loss=0.536\n",
      "  batch  640: loss=0.594\n",
      "  batch  660: loss=0.590\n",
      "  batch  680: loss=0.591\n",
      "  batch  700: loss=0.558\n",
      "  batch  720: loss=0.582\n",
      "  batch  740: loss=0.581\n",
      "  batch  760: loss=0.584\n",
      "  batch  780: loss=0.601\n",
      "Testing on validation set\n",
      "  acc=0.796\n",
      "Training epoch 2\n",
      "  batch   20: loss=0.587\n",
      "  batch   40: loss=0.540\n",
      "  batch   60: loss=0.523\n",
      "  batch   80: loss=0.549\n",
      "  batch  100: loss=0.521\n",
      "  batch  120: loss=0.559\n",
      "  batch  140: loss=0.527\n",
      "  batch  160: loss=0.504\n",
      "  batch  180: loss=0.567\n",
      "  batch  200: loss=0.509\n",
      "  batch  220: loss=0.541\n",
      "  batch  240: loss=0.548\n",
      "  batch  260: loss=0.594\n",
      "  batch  280: loss=0.534\n",
      "  batch  300: loss=0.574\n",
      "  batch  320: loss=0.531\n",
      "  batch  340: loss=0.528\n",
      "  batch  360: loss=0.530\n",
      "  batch  380: loss=0.537\n",
      "  batch  400: loss=0.525\n",
      "  batch  420: loss=0.535\n",
      "  batch  440: loss=0.496\n",
      "  batch  460: loss=0.551\n",
      "  batch  480: loss=0.548\n",
      "  batch  500: loss=0.545\n",
      "  batch  520: loss=0.527\n",
      "  batch  540: loss=0.554\n",
      "  batch  560: loss=0.515\n",
      "  batch  580: loss=0.495\n",
      "  batch  600: loss=0.529\n",
      "  batch  620: loss=0.529\n",
      "  batch  640: loss=0.523\n",
      "  batch  660: loss=0.528\n",
      "  batch  680: loss=0.500\n",
      "  batch  700: loss=0.535\n",
      "  batch  720: loss=0.492\n",
      "  batch  740: loss=0.527\n",
      "  batch  760: loss=0.542\n",
      "  batch  780: loss=0.485\n",
      "Testing on validation set\n",
      "  acc=0.819\n",
      "Training epoch 3\n",
      "  batch   20: loss=0.529\n",
      "  batch   40: loss=0.566\n",
      "  batch   60: loss=0.497\n",
      "  batch   80: loss=0.486\n",
      "  batch  100: loss=0.492\n",
      "  batch  120: loss=0.510\n",
      "  batch  140: loss=0.487\n",
      "  batch  160: loss=0.456\n",
      "  batch  180: loss=0.486\n",
      "  batch  200: loss=0.503\n",
      "  batch  220: loss=0.490\n",
      "  batch  240: loss=0.531\n",
      "  batch  260: loss=0.482\n",
      "  batch  280: loss=0.543\n",
      "  batch  300: loss=0.472\n",
      "  batch  320: loss=0.515\n",
      "  batch  340: loss=0.481\n",
      "  batch  360: loss=0.553\n",
      "  batch  380: loss=0.522\n",
      "  batch  400: loss=0.472\n",
      "  batch  420: loss=0.504\n",
      "  batch  440: loss=0.470\n",
      "  batch  460: loss=0.520\n",
      "  batch  480: loss=0.486\n",
      "  batch  500: loss=0.483\n",
      "  batch  520: loss=0.488\n",
      "  batch  540: loss=0.502\n",
      "  batch  560: loss=0.506\n",
      "  batch  580: loss=0.430\n",
      "  batch  600: loss=0.467\n",
      "  batch  620: loss=0.477\n",
      "  batch  640: loss=0.477\n",
      "  batch  660: loss=0.491\n",
      "  batch  680: loss=0.452\n",
      "  batch  700: loss=0.491\n",
      "  batch  720: loss=0.471\n",
      "  batch  740: loss=0.500\n",
      "  batch  760: loss=0.416\n",
      "  batch  780: loss=0.478\n",
      "Testing on validation set\n",
      "  acc=0.827\n",
      "Training epoch 4\n",
      "  batch   20: loss=0.450\n",
      "  batch   40: loss=0.479\n",
      "  batch   60: loss=0.506\n",
      "  batch   80: loss=0.491\n",
      "  batch  100: loss=0.465\n",
      "  batch  120: loss=0.470\n",
      "  batch  140: loss=0.496\n",
      "  batch  160: loss=0.437\n",
      "  batch  180: loss=0.498\n",
      "  batch  200: loss=0.446\n",
      "  batch  220: loss=0.455\n",
      "  batch  240: loss=0.478\n",
      "  batch  260: loss=0.512\n",
      "  batch  280: loss=0.495\n",
      "  batch  300: loss=0.455\n",
      "  batch  320: loss=0.470\n",
      "  batch  340: loss=0.472\n",
      "  batch  360: loss=0.469\n",
      "  batch  380: loss=0.476\n",
      "  batch  400: loss=0.462\n",
      "  batch  420: loss=0.457\n",
      "  batch  440: loss=0.447\n",
      "  batch  460: loss=0.479\n",
      "  batch  480: loss=0.413\n",
      "  batch  500: loss=0.420\n",
      "  batch  520: loss=0.460\n",
      "  batch  540: loss=0.477\n",
      "  batch  560: loss=0.477\n",
      "  batch  580: loss=0.464\n",
      "  batch  600: loss=0.458\n",
      "  batch  620: loss=0.444\n",
      "  batch  640: loss=0.441\n",
      "  batch  660: loss=0.451\n",
      "  batch  680: loss=0.481\n",
      "  batch  700: loss=0.483\n",
      "  batch  720: loss=0.429\n",
      "  batch  740: loss=0.456\n",
      "  batch  760: loss=0.487\n",
      "  batch  780: loss=0.463\n",
      "Testing on validation set\n",
      "  acc=0.833\n",
      "Training epoch 5\n",
      "  batch   20: loss=0.417\n",
      "  batch   40: loss=0.456\n",
      "  batch   60: loss=0.472\n",
      "  batch   80: loss=0.445\n",
      "  batch  100: loss=0.465\n",
      "  batch  120: loss=0.433\n",
      "  batch  140: loss=0.469\n",
      "  batch  160: loss=0.479\n",
      "  batch  180: loss=0.470\n",
      "  batch  200: loss=0.430\n",
      "  batch  220: loss=0.452\n",
      "  batch  240: loss=0.481\n",
      "  batch  260: loss=0.437\n",
      "  batch  280: loss=0.437\n",
      "  batch  300: loss=0.444\n",
      "  batch  320: loss=0.442\n",
      "  batch  340: loss=0.448\n",
      "  batch  360: loss=0.455\n",
      "  batch  380: loss=0.475\n",
      "  batch  400: loss=0.438\n",
      "  batch  420: loss=0.471\n",
      "  batch  440: loss=0.470\n",
      "  batch  460: loss=0.488\n",
      "  batch  480: loss=0.429\n",
      "  batch  500: loss=0.480\n",
      "  batch  520: loss=0.415\n",
      "  batch  540: loss=0.439\n",
      "  batch  560: loss=0.433\n",
      "  batch  580: loss=0.406\n",
      "  batch  600: loss=0.441\n",
      "  batch  620: loss=0.445\n",
      "  batch  640: loss=0.452\n",
      "  batch  660: loss=0.448\n",
      "  batch  680: loss=0.413\n",
      "  batch  700: loss=0.427\n",
      "  batch  720: loss=0.429\n",
      "  batch  740: loss=0.426\n",
      "  batch  760: loss=0.419\n",
      "  batch  780: loss=0.424\n",
      "Testing on validation set\n",
      "  acc=0.831\n",
      "Training epoch 6\n",
      "  batch   20: loss=0.444\n",
      "  batch   40: loss=0.443\n",
      "  batch   60: loss=0.500\n",
      "  batch   80: loss=0.443\n",
      "  batch  100: loss=0.423\n",
      "  batch  120: loss=0.435\n",
      "  batch  140: loss=0.412\n",
      "  batch  160: loss=0.414\n",
      "  batch  180: loss=0.440\n",
      "  batch  200: loss=0.428\n",
      "  batch  220: loss=0.433\n",
      "  batch  240: loss=0.406\n",
      "  batch  260: loss=0.459\n",
      "  batch  280: loss=0.401\n",
      "  batch  300: loss=0.443\n",
      "  batch  320: loss=0.441\n",
      "  batch  340: loss=0.419\n",
      "  batch  360: loss=0.443\n",
      "  batch  380: loss=0.435\n",
      "  batch  400: loss=0.455\n",
      "  batch  420: loss=0.388\n",
      "  batch  440: loss=0.450\n",
      "  batch  460: loss=0.410\n",
      "  batch  480: loss=0.437\n",
      "  batch  500: loss=0.472\n",
      "  batch  520: loss=0.491\n",
      "  batch  540: loss=0.417\n",
      "  batch  560: loss=0.433\n",
      "  batch  580: loss=0.419\n",
      "  batch  600: loss=0.413\n",
      "  batch  620: loss=0.451\n",
      "  batch  640: loss=0.407\n",
      "  batch  660: loss=0.400\n",
      "  batch  680: loss=0.399\n",
      "  batch  700: loss=0.439\n",
      "  batch  720: loss=0.414\n",
      "  batch  740: loss=0.406\n",
      "  batch  760: loss=0.393\n",
      "  batch  780: loss=0.410\n",
      "Testing on validation set\n",
      "  acc=0.847\n",
      "Training epoch 7\n",
      "  batch   20: loss=0.406\n",
      "  batch   40: loss=0.403\n",
      "  batch   60: loss=0.423\n",
      "  batch   80: loss=0.425\n",
      "  batch  100: loss=0.401\n",
      "  batch  120: loss=0.422\n",
      "  batch  140: loss=0.389\n",
      "  batch  160: loss=0.457\n",
      "  batch  180: loss=0.435\n",
      "  batch  200: loss=0.379\n",
      "  batch  220: loss=0.399\n",
      "  batch  240: loss=0.425\n",
      "  batch  260: loss=0.434\n",
      "  batch  280: loss=0.405\n",
      "  batch  300: loss=0.417\n",
      "  batch  320: loss=0.410\n",
      "  batch  340: loss=0.413\n",
      "  batch  360: loss=0.447\n",
      "  batch  380: loss=0.422\n",
      "  batch  400: loss=0.392\n",
      "  batch  420: loss=0.421\n",
      "  batch  440: loss=0.413\n",
      "  batch  460: loss=0.422\n",
      "  batch  480: loss=0.357\n",
      "  batch  500: loss=0.381\n",
      "  batch  520: loss=0.428\n",
      "  batch  540: loss=0.438\n",
      "  batch  560: loss=0.423\n",
      "  batch  580: loss=0.439\n",
      "  batch  600: loss=0.430\n",
      "  batch  620: loss=0.390\n",
      "  batch  640: loss=0.394\n",
      "  batch  660: loss=0.375\n",
      "  batch  680: loss=0.435\n",
      "  batch  700: loss=0.427\n",
      "  batch  720: loss=0.429\n",
      "  batch  740: loss=0.413\n",
      "  batch  760: loss=0.425\n",
      "  batch  780: loss=0.389\n",
      "Testing on validation set\n",
      "  acc=0.852\n",
      "Training epoch 8\n",
      "  batch   20: loss=0.424\n",
      "  batch   40: loss=0.438\n",
      "  batch   60: loss=0.423\n",
      "  batch   80: loss=0.420\n",
      "  batch  100: loss=0.385\n",
      "  batch  120: loss=0.376\n",
      "  batch  140: loss=0.367\n",
      "  batch  160: loss=0.380\n",
      "  batch  180: loss=0.413\n",
      "  batch  200: loss=0.403\n",
      "  batch  220: loss=0.358\n",
      "  batch  240: loss=0.392\n",
      "  batch  260: loss=0.445\n",
      "  batch  280: loss=0.424\n",
      "  batch  300: loss=0.400\n",
      "  batch  320: loss=0.401\n",
      "  batch  340: loss=0.386\n",
      "  batch  360: loss=0.421\n",
      "  batch  380: loss=0.430\n",
      "  batch  400: loss=0.389\n",
      "  batch  420: loss=0.427\n",
      "  batch  440: loss=0.444\n",
      "  batch  460: loss=0.393\n",
      "  batch  480: loss=0.396\n",
      "  batch  500: loss=0.368\n",
      "  batch  520: loss=0.400\n",
      "  batch  540: loss=0.372\n",
      "  batch  560: loss=0.403\n",
      "  batch  580: loss=0.420\n",
      "  batch  600: loss=0.401\n",
      "  batch  620: loss=0.395\n",
      "  batch  640: loss=0.364\n",
      "  batch  660: loss=0.423\n",
      "  batch  680: loss=0.449\n",
      "  batch  700: loss=0.378\n",
      "  batch  720: loss=0.396\n",
      "  batch  740: loss=0.379\n",
      "  batch  760: loss=0.364\n",
      "  batch  780: loss=0.397\n",
      "Testing on validation set\n",
      "  acc=0.848\n",
      "Training epoch 9\n",
      "  batch   20: loss=0.408\n",
      "  batch   40: loss=0.393\n",
      "  batch   60: loss=0.371\n",
      "  batch   80: loss=0.398\n",
      "  batch  100: loss=0.377\n",
      "  batch  120: loss=0.396\n",
      "  batch  140: loss=0.423\n",
      "  batch  160: loss=0.389\n",
      "  batch  180: loss=0.376\n",
      "  batch  200: loss=0.430\n",
      "  batch  220: loss=0.398\n",
      "  batch  240: loss=0.370\n",
      "  batch  260: loss=0.371\n",
      "  batch  280: loss=0.404\n",
      "  batch  300: loss=0.448\n",
      "  batch  320: loss=0.405\n",
      "  batch  340: loss=0.388\n",
      "  batch  360: loss=0.381\n",
      "  batch  380: loss=0.365\n",
      "  batch  400: loss=0.406\n",
      "  batch  420: loss=0.406\n",
      "  batch  440: loss=0.412\n",
      "  batch  460: loss=0.370\n",
      "  batch  480: loss=0.351\n",
      "  batch  500: loss=0.386\n",
      "  batch  520: loss=0.405\n",
      "  batch  540: loss=0.419\n",
      "  batch  560: loss=0.382\n",
      "  batch  580: loss=0.343\n",
      "  batch  600: loss=0.384\n",
      "  batch  620: loss=0.386\n",
      "  batch  640: loss=0.377\n",
      "  batch  660: loss=0.409\n",
      "  batch  680: loss=0.378\n",
      "  batch  700: loss=0.380\n",
      "  batch  720: loss=0.380\n",
      "  batch  740: loss=0.398\n",
      "  batch  760: loss=0.332\n",
      "  batch  780: loss=0.398\n",
      "Testing on validation set\n",
      "  acc=0.859\n"
     ]
    }
   ],
   "source": [
    "args.lr = 0.001\n",
    "\n",
    "model3 = Net()\n",
    "model3.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model3.parameters(),\n",
    "    lr=args.lr,\n",
    "    momentum=args.momentum\n",
    ")\n",
    "\n",
    "for e in range(args.num_epochs):\n",
    "  print('Training epoch {}'.format(e))\n",
    "  train(args, model3, criterion, train_loader, optimizer, device)\n",
    "  print('Testing on validation set')\n",
    "  test(args, model3, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UwWeMJt44hnP"
   },
   "source": [
    "The lower learning rate slightly decreases the validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpv68lsu6xMH"
   },
   "source": [
    "Part 4: Adding more channels from layer 1 to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Pj76Mnti6wfa",
    "outputId": "3f1c23b8-38af-4813-f75c-a4351ed3e237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "  batch   20: loss=2.113\n",
      "  batch   40: loss=1.215\n",
      "  batch   60: loss=0.864\n",
      "  batch   80: loss=0.770\n",
      "  batch  100: loss=0.762\n",
      "  batch  120: loss=0.706\n",
      "  batch  140: loss=0.684\n",
      "  batch  160: loss=0.641\n",
      "  batch  180: loss=0.572\n",
      "  batch  200: loss=0.573\n",
      "  batch  220: loss=0.580\n",
      "  batch  240: loss=0.548\n",
      "  batch  260: loss=0.473\n",
      "  batch  280: loss=0.506\n",
      "  batch  300: loss=0.533\n",
      "  batch  320: loss=0.503\n",
      "  batch  340: loss=0.508\n",
      "  batch  360: loss=0.550\n",
      "  batch  380: loss=0.474\n",
      "  batch  400: loss=0.440\n",
      "  batch  420: loss=0.460\n",
      "  batch  440: loss=0.458\n",
      "  batch  460: loss=0.423\n",
      "  batch  480: loss=0.478\n",
      "  batch  500: loss=0.481\n",
      "  batch  520: loss=0.438\n",
      "  batch  540: loss=0.444\n",
      "  batch  560: loss=0.397\n",
      "  batch  580: loss=0.409\n",
      "  batch  600: loss=0.449\n",
      "  batch  620: loss=0.436\n",
      "  batch  640: loss=0.425\n",
      "  batch  660: loss=0.388\n",
      "  batch  680: loss=0.447\n",
      "  batch  700: loss=0.371\n",
      "  batch  720: loss=0.458\n",
      "  batch  740: loss=0.416\n",
      "  batch  760: loss=0.391\n",
      "  batch  780: loss=0.399\n",
      "Testing on validation set\n",
      "  acc=0.845\n",
      "Training epoch 1\n",
      "  batch   20: loss=0.394\n",
      "  batch   40: loss=0.356\n",
      "  batch   60: loss=0.387\n",
      "  batch   80: loss=0.433\n",
      "  batch  100: loss=0.402\n",
      "  batch  120: loss=0.331\n",
      "  batch  140: loss=0.347\n",
      "  batch  160: loss=0.346\n",
      "  batch  180: loss=0.431\n",
      "  batch  200: loss=0.354\n",
      "  batch  220: loss=0.374\n",
      "  batch  240: loss=0.353\n",
      "  batch  260: loss=0.329\n",
      "  batch  280: loss=0.408\n",
      "  batch  300: loss=0.422\n",
      "  batch  320: loss=0.363\n",
      "  batch  340: loss=0.373\n",
      "  batch  360: loss=0.373\n",
      "  batch  380: loss=0.378\n",
      "  batch  400: loss=0.383\n",
      "  batch  420: loss=0.387\n",
      "  batch  440: loss=0.360\n",
      "  batch  460: loss=0.367\n",
      "  batch  480: loss=0.374\n",
      "  batch  500: loss=0.339\n",
      "  batch  520: loss=0.359\n",
      "  batch  540: loss=0.366\n",
      "  batch  560: loss=0.370\n",
      "  batch  580: loss=0.364\n",
      "  batch  600: loss=0.392\n",
      "  batch  620: loss=0.341\n",
      "  batch  640: loss=0.356\n",
      "  batch  660: loss=0.363\n",
      "  batch  680: loss=0.350\n",
      "  batch  700: loss=0.357\n",
      "  batch  720: loss=0.375\n",
      "  batch  740: loss=0.347\n",
      "  batch  760: loss=0.329\n",
      "  batch  780: loss=0.338\n",
      "Testing on validation set\n",
      "  acc=0.870\n",
      "Training epoch 2\n",
      "  batch   20: loss=0.373\n",
      "  batch   40: loss=0.341\n",
      "  batch   60: loss=0.325\n",
      "  batch   80: loss=0.305\n",
      "  batch  100: loss=0.323\n",
      "  batch  120: loss=0.347\n",
      "  batch  140: loss=0.357\n",
      "  batch  160: loss=0.306\n",
      "  batch  180: loss=0.349\n",
      "  batch  200: loss=0.326\n",
      "  batch  220: loss=0.315\n",
      "  batch  240: loss=0.385\n",
      "  batch  260: loss=0.324\n",
      "  batch  280: loss=0.309\n",
      "  batch  300: loss=0.312\n",
      "  batch  320: loss=0.295\n",
      "  batch  340: loss=0.316\n",
      "  batch  360: loss=0.352\n",
      "  batch  380: loss=0.371\n",
      "  batch  400: loss=0.331\n",
      "  batch  420: loss=0.336\n",
      "  batch  440: loss=0.326\n",
      "  batch  460: loss=0.338\n",
      "  batch  480: loss=0.321\n",
      "  batch  500: loss=0.335\n",
      "  batch  520: loss=0.361\n",
      "  batch  540: loss=0.314\n",
      "  batch  560: loss=0.319\n",
      "  batch  580: loss=0.331\n",
      "  batch  600: loss=0.325\n",
      "  batch  620: loss=0.342\n",
      "  batch  640: loss=0.326\n",
      "  batch  660: loss=0.282\n",
      "  batch  680: loss=0.298\n",
      "  batch  700: loss=0.304\n",
      "  batch  720: loss=0.323\n",
      "  batch  740: loss=0.316\n",
      "  batch  760: loss=0.321\n",
      "  batch  780: loss=0.363\n",
      "Testing on validation set\n",
      "  acc=0.880\n",
      "Training epoch 3\n",
      "  batch   20: loss=0.337\n",
      "  batch   40: loss=0.320\n",
      "  batch   60: loss=0.319\n",
      "  batch   80: loss=0.254\n",
      "  batch  100: loss=0.314\n",
      "  batch  120: loss=0.311\n",
      "  batch  140: loss=0.303\n",
      "  batch  160: loss=0.284\n",
      "  batch  180: loss=0.291\n",
      "  batch  200: loss=0.270\n",
      "  batch  220: loss=0.317\n",
      "  batch  240: loss=0.295\n",
      "  batch  260: loss=0.300\n",
      "  batch  280: loss=0.320\n",
      "  batch  300: loss=0.351\n",
      "  batch  320: loss=0.284\n",
      "  batch  340: loss=0.251\n",
      "  batch  360: loss=0.307\n",
      "  batch  380: loss=0.314\n",
      "  batch  400: loss=0.328\n",
      "  batch  420: loss=0.302\n",
      "  batch  440: loss=0.328\n",
      "  batch  460: loss=0.311\n",
      "  batch  480: loss=0.303\n",
      "  batch  500: loss=0.279\n",
      "  batch  520: loss=0.335\n",
      "  batch  540: loss=0.270\n",
      "  batch  560: loss=0.336\n",
      "  batch  580: loss=0.298\n",
      "  batch  600: loss=0.316\n",
      "  batch  620: loss=0.299\n",
      "  batch  640: loss=0.295\n",
      "  batch  660: loss=0.325\n",
      "  batch  680: loss=0.313\n",
      "  batch  700: loss=0.303\n",
      "  batch  720: loss=0.324\n",
      "  batch  740: loss=0.325\n",
      "  batch  760: loss=0.293\n",
      "  batch  780: loss=0.283\n",
      "Testing on validation set\n",
      "  acc=0.890\n",
      "Training epoch 4\n",
      "  batch   20: loss=0.289\n",
      "  batch   40: loss=0.241\n",
      "  batch   60: loss=0.327\n",
      "  batch   80: loss=0.286\n",
      "  batch  100: loss=0.280\n",
      "  batch  120: loss=0.293\n",
      "  batch  140: loss=0.260\n",
      "  batch  160: loss=0.295\n",
      "  batch  180: loss=0.328\n",
      "  batch  200: loss=0.263\n",
      "  batch  220: loss=0.288\n",
      "  batch  240: loss=0.266\n",
      "  batch  260: loss=0.281\n",
      "  batch  280: loss=0.284\n",
      "  batch  300: loss=0.288\n",
      "  batch  320: loss=0.287\n",
      "  batch  340: loss=0.285\n",
      "  batch  360: loss=0.245\n",
      "  batch  380: loss=0.318\n",
      "  batch  400: loss=0.296\n",
      "  batch  420: loss=0.313\n",
      "  batch  440: loss=0.330\n",
      "  batch  460: loss=0.280\n",
      "  batch  480: loss=0.327\n",
      "  batch  500: loss=0.291\n",
      "  batch  520: loss=0.283\n",
      "  batch  540: loss=0.329\n",
      "  batch  560: loss=0.305\n",
      "  batch  580: loss=0.266\n",
      "  batch  600: loss=0.275\n",
      "  batch  620: loss=0.279\n",
      "  batch  640: loss=0.284\n",
      "  batch  660: loss=0.307\n",
      "  batch  680: loss=0.260\n",
      "  batch  700: loss=0.272\n",
      "  batch  720: loss=0.281\n",
      "  batch  740: loss=0.280\n",
      "  batch  760: loss=0.259\n",
      "  batch  780: loss=0.318\n",
      "Testing on validation set\n",
      "  acc=0.882\n",
      "Training epoch 5\n",
      "  batch   20: loss=0.253\n",
      "  batch   40: loss=0.298\n",
      "  batch   60: loss=0.269\n",
      "  batch   80: loss=0.281\n",
      "  batch  100: loss=0.274\n",
      "  batch  120: loss=0.279\n",
      "  batch  140: loss=0.269\n",
      "  batch  160: loss=0.264\n",
      "  batch  180: loss=0.271\n",
      "  batch  200: loss=0.300\n",
      "  batch  220: loss=0.274\n",
      "  batch  240: loss=0.262\n",
      "  batch  260: loss=0.286\n",
      "  batch  280: loss=0.288\n",
      "  batch  300: loss=0.253\n",
      "  batch  320: loss=0.234\n",
      "  batch  340: loss=0.265\n",
      "  batch  360: loss=0.272\n",
      "  batch  380: loss=0.289\n",
      "  batch  400: loss=0.281\n",
      "  batch  420: loss=0.285\n",
      "  batch  440: loss=0.307\n",
      "  batch  460: loss=0.305\n",
      "  batch  480: loss=0.255\n",
      "  batch  500: loss=0.262\n",
      "  batch  520: loss=0.288\n",
      "  batch  540: loss=0.250\n",
      "  batch  560: loss=0.296\n",
      "  batch  580: loss=0.235\n",
      "  batch  600: loss=0.290\n",
      "  batch  620: loss=0.255\n",
      "  batch  640: loss=0.270\n",
      "  batch  660: loss=0.277\n",
      "  batch  680: loss=0.298\n",
      "  batch  700: loss=0.289\n",
      "  batch  720: loss=0.273\n",
      "  batch  740: loss=0.277\n",
      "  batch  760: loss=0.264\n",
      "  batch  780: loss=0.276\n",
      "Testing on validation set\n",
      "  acc=0.892\n",
      "Training epoch 6\n",
      "  batch   20: loss=0.294\n",
      "  batch   40: loss=0.290\n",
      "  batch   60: loss=0.246\n",
      "  batch   80: loss=0.270\n",
      "  batch  100: loss=0.260\n",
      "  batch  120: loss=0.254\n",
      "  batch  140: loss=0.275\n",
      "  batch  160: loss=0.246\n",
      "  batch  180: loss=0.237\n",
      "  batch  200: loss=0.241\n",
      "  batch  220: loss=0.268\n",
      "  batch  240: loss=0.269\n",
      "  batch  260: loss=0.262\n",
      "  batch  280: loss=0.238\n",
      "  batch  300: loss=0.242\n",
      "  batch  320: loss=0.268\n",
      "  batch  340: loss=0.249\n",
      "  batch  360: loss=0.216\n",
      "  batch  380: loss=0.297\n",
      "  batch  400: loss=0.251\n",
      "  batch  420: loss=0.299\n",
      "  batch  440: loss=0.289\n",
      "  batch  460: loss=0.262\n",
      "  batch  480: loss=0.238\n",
      "  batch  500: loss=0.260\n",
      "  batch  520: loss=0.222\n",
      "  batch  540: loss=0.263\n",
      "  batch  560: loss=0.287\n",
      "  batch  580: loss=0.270\n",
      "  batch  600: loss=0.268\n",
      "  batch  620: loss=0.242\n",
      "  batch  640: loss=0.256\n",
      "  batch  660: loss=0.276\n",
      "  batch  680: loss=0.262\n",
      "  batch  700: loss=0.231\n",
      "  batch  720: loss=0.236\n",
      "  batch  740: loss=0.303\n",
      "  batch  760: loss=0.262\n",
      "  batch  780: loss=0.283\n",
      "Testing on validation set\n",
      "  acc=0.889\n",
      "Training epoch 7\n",
      "  batch   20: loss=0.270\n",
      "  batch   40: loss=0.251\n",
      "  batch   60: loss=0.249\n",
      "  batch   80: loss=0.243\n",
      "  batch  100: loss=0.235\n",
      "  batch  120: loss=0.257\n",
      "  batch  140: loss=0.268\n",
      "  batch  160: loss=0.271\n",
      "  batch  180: loss=0.250\n",
      "  batch  200: loss=0.253\n",
      "  batch  220: loss=0.206\n",
      "  batch  240: loss=0.267\n",
      "  batch  260: loss=0.277\n",
      "  batch  280: loss=0.231\n",
      "  batch  300: loss=0.255\n",
      "  batch  320: loss=0.271\n",
      "  batch  340: loss=0.263\n",
      "  batch  360: loss=0.240\n",
      "  batch  380: loss=0.258\n",
      "  batch  400: loss=0.236\n",
      "  batch  420: loss=0.281\n",
      "  batch  440: loss=0.278\n",
      "  batch  460: loss=0.235\n",
      "  batch  480: loss=0.240\n",
      "  batch  500: loss=0.196\n",
      "  batch  520: loss=0.305\n",
      "  batch  540: loss=0.263\n",
      "  batch  560: loss=0.221\n",
      "  batch  580: loss=0.233\n",
      "  batch  600: loss=0.242\n",
      "  batch  620: loss=0.262\n",
      "  batch  640: loss=0.280\n",
      "  batch  660: loss=0.266\n",
      "  batch  680: loss=0.291\n",
      "  batch  700: loss=0.251\n",
      "  batch  720: loss=0.247\n",
      "  batch  740: loss=0.240\n",
      "  batch  760: loss=0.272\n",
      "  batch  780: loss=0.220\n",
      "Testing on validation set\n",
      "  acc=0.899\n",
      "Training epoch 8\n",
      "  batch   20: loss=0.273\n",
      "  batch   40: loss=0.261\n",
      "  batch   60: loss=0.218\n",
      "  batch   80: loss=0.259\n",
      "  batch  100: loss=0.245\n",
      "  batch  120: loss=0.256\n",
      "  batch  140: loss=0.193\n",
      "  batch  160: loss=0.257\n",
      "  batch  180: loss=0.216\n",
      "  batch  200: loss=0.287\n",
      "  batch  220: loss=0.256\n",
      "  batch  240: loss=0.207\n",
      "  batch  260: loss=0.236\n",
      "  batch  280: loss=0.235\n",
      "  batch  300: loss=0.220\n",
      "  batch  320: loss=0.251\n",
      "  batch  340: loss=0.231\n",
      "  batch  360: loss=0.216\n",
      "  batch  380: loss=0.223\n",
      "  batch  400: loss=0.259\n",
      "  batch  420: loss=0.255\n",
      "  batch  440: loss=0.252\n",
      "  batch  460: loss=0.237\n",
      "  batch  480: loss=0.242\n",
      "  batch  500: loss=0.270\n",
      "  batch  520: loss=0.223\n",
      "  batch  540: loss=0.236\n",
      "  batch  560: loss=0.210\n",
      "  batch  580: loss=0.259\n",
      "  batch  600: loss=0.253\n",
      "  batch  620: loss=0.253\n",
      "  batch  640: loss=0.250\n",
      "  batch  660: loss=0.248\n",
      "  batch  680: loss=0.245\n",
      "  batch  700: loss=0.230\n",
      "  batch  720: loss=0.239\n",
      "  batch  740: loss=0.301\n",
      "  batch  760: loss=0.257\n",
      "  batch  780: loss=0.256\n",
      "Testing on validation set\n",
      "  acc=0.898\n",
      "Training epoch 9\n",
      "  batch   20: loss=0.229\n",
      "  batch   40: loss=0.252\n",
      "  batch   60: loss=0.226\n",
      "  batch   80: loss=0.244\n",
      "  batch  100: loss=0.239\n",
      "  batch  120: loss=0.230\n",
      "  batch  140: loss=0.229\n",
      "  batch  160: loss=0.219\n",
      "  batch  180: loss=0.257\n",
      "  batch  200: loss=0.248\n",
      "  batch  220: loss=0.230\n",
      "  batch  240: loss=0.224\n",
      "  batch  260: loss=0.193\n",
      "  batch  280: loss=0.231\n",
      "  batch  300: loss=0.236\n",
      "  batch  320: loss=0.232\n",
      "  batch  340: loss=0.244\n",
      "  batch  360: loss=0.250\n",
      "  batch  380: loss=0.207\n",
      "  batch  400: loss=0.234\n",
      "  batch  420: loss=0.223\n",
      "  batch  440: loss=0.261\n",
      "  batch  460: loss=0.167\n",
      "  batch  480: loss=0.233\n",
      "  batch  500: loss=0.240\n",
      "  batch  520: loss=0.230\n",
      "  batch  540: loss=0.256\n",
      "  batch  560: loss=0.226\n",
      "  batch  580: loss=0.269\n",
      "  batch  600: loss=0.228\n",
      "  batch  620: loss=0.247\n",
      "  batch  640: loss=0.247\n",
      "  batch  660: loss=0.264\n",
      "  batch  680: loss=0.243\n",
      "  batch  700: loss=0.238\n",
      "  batch  720: loss=0.245\n",
      "  batch  740: loss=0.259\n",
      "  batch  760: loss=0.248\n",
      "  batch  780: loss=0.208\n",
      "Testing on validation set\n",
      "  acc=0.901\n"
     ]
    }
   ],
   "source": [
    "class Net2(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net2, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "    self.conv2 = nn.Conv2d(32, 16, 5)\n",
    "    self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    self._initialize_weights()\n",
    "  \n",
    "  def _initialize_weights(self):\n",
    "    pass\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.conv1(x))\n",
    "    x = F.max_pool2d(x, 2, stride=2)\n",
    "    x = F.relu(self.conv2(x))\n",
    "    x = F.max_pool2d(x, 2, stride=2)\n",
    "    x = torch.flatten(x, start_dim=1)\n",
    "    x = self.fc3(x)\n",
    "    return x\n",
    "\n",
    "args.lr = 0.01\n",
    "\n",
    "model4 = Net2()\n",
    "model4.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model4.parameters(),\n",
    "    lr=args.lr,\n",
    "    momentum=args.momentum\n",
    ")\n",
    "\n",
    "for e in range(args.num_epochs):\n",
    "  print('Training epoch {}'.format(e))\n",
    "  train(args, model4, criterion, train_loader, optimizer, device)\n",
    "  print('Testing on validation set')\n",
    "  test(args, model4, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GWJa7hcCPFez"
   },
   "source": [
    "Increasing the number of channels for the convolutional layers increases the accuracy on the validation set by a good amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4mxOdFChQ8Iy"
   },
   "source": [
    "Part 5: Dropout layer after the first convolutional layer for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jwj70sBPQw7G",
    "outputId": "251b9993-0d6b-4ee8-e79f-7182dad21109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "  batch   20: loss=2.127\n",
      "  batch   40: loss=1.233\n",
      "  batch   60: loss=0.976\n",
      "  batch   80: loss=0.865\n",
      "  batch  100: loss=0.712\n",
      "  batch  120: loss=0.643\n",
      "  batch  140: loss=0.619\n",
      "  batch  160: loss=0.682\n",
      "  batch  180: loss=0.621\n",
      "  batch  200: loss=0.607\n",
      "  batch  220: loss=0.584\n",
      "  batch  240: loss=0.616\n",
      "  batch  260: loss=0.586\n",
      "  batch  280: loss=0.589\n",
      "  batch  300: loss=0.515\n",
      "  batch  320: loss=0.546\n",
      "  batch  340: loss=0.527\n",
      "  batch  360: loss=0.550\n",
      "  batch  380: loss=0.517\n",
      "  batch  400: loss=0.493\n",
      "  batch  420: loss=0.517\n",
      "  batch  440: loss=0.508\n",
      "  batch  460: loss=0.466\n",
      "  batch  480: loss=0.468\n",
      "  batch  500: loss=0.492\n",
      "  batch  520: loss=0.465\n",
      "  batch  540: loss=0.440\n",
      "  batch  560: loss=0.463\n",
      "  batch  580: loss=0.446\n",
      "  batch  600: loss=0.448\n",
      "  batch  620: loss=0.466\n",
      "  batch  640: loss=0.415\n",
      "  batch  660: loss=0.473\n",
      "  batch  680: loss=0.426\n",
      "  batch  700: loss=0.454\n",
      "  batch  720: loss=0.458\n",
      "  batch  740: loss=0.474\n",
      "  batch  760: loss=0.433\n",
      "  batch  780: loss=0.430\n",
      "Testing on validation set\n",
      "  acc=0.842\n",
      "Training epoch 1\n",
      "  batch   20: loss=0.407\n",
      "  batch   40: loss=0.446\n",
      "  batch   60: loss=0.476\n",
      "  batch   80: loss=0.437\n",
      "  batch  100: loss=0.430\n",
      "  batch  120: loss=0.409\n",
      "  batch  140: loss=0.469\n",
      "  batch  160: loss=0.419\n",
      "  batch  180: loss=0.418\n",
      "  batch  200: loss=0.350\n",
      "  batch  220: loss=0.357\n",
      "  batch  240: loss=0.422\n",
      "  batch  260: loss=0.476\n",
      "  batch  280: loss=0.443\n",
      "  batch  300: loss=0.419\n",
      "  batch  320: loss=0.400\n",
      "  batch  340: loss=0.388\n",
      "  batch  360: loss=0.402\n",
      "  batch  380: loss=0.388\n",
      "  batch  400: loss=0.382\n",
      "  batch  420: loss=0.358\n",
      "  batch  440: loss=0.419\n",
      "  batch  460: loss=0.376\n",
      "  batch  480: loss=0.383\n",
      "  batch  500: loss=0.395\n",
      "  batch  520: loss=0.393\n",
      "  batch  540: loss=0.390\n",
      "  batch  560: loss=0.387\n",
      "  batch  580: loss=0.360\n",
      "  batch  600: loss=0.416\n",
      "  batch  620: loss=0.398\n",
      "  batch  640: loss=0.368\n",
      "  batch  660: loss=0.344\n",
      "  batch  680: loss=0.388\n",
      "  batch  700: loss=0.376\n",
      "  batch  720: loss=0.371\n",
      "  batch  740: loss=0.380\n",
      "  batch  760: loss=0.394\n",
      "  batch  780: loss=0.409\n",
      "Testing on validation set\n",
      "  acc=0.870\n",
      "Training epoch 2\n",
      "  batch   20: loss=0.360\n",
      "  batch   40: loss=0.377\n",
      "  batch   60: loss=0.365\n",
      "  batch   80: loss=0.351\n",
      "  batch  100: loss=0.373\n",
      "  batch  120: loss=0.383\n",
      "  batch  140: loss=0.346\n",
      "  batch  160: loss=0.367\n",
      "  batch  180: loss=0.348\n",
      "  batch  200: loss=0.402\n",
      "  batch  220: loss=0.316\n",
      "  batch  240: loss=0.330\n",
      "  batch  260: loss=0.386\n",
      "  batch  280: loss=0.362\n",
      "  batch  300: loss=0.320\n",
      "  batch  320: loss=0.375\n",
      "  batch  340: loss=0.377\n",
      "  batch  360: loss=0.398\n",
      "  batch  380: loss=0.351\n",
      "  batch  400: loss=0.382\n",
      "  batch  420: loss=0.409\n",
      "  batch  440: loss=0.332\n",
      "  batch  460: loss=0.328\n",
      "  batch  480: loss=0.392\n",
      "  batch  500: loss=0.348\n",
      "  batch  520: loss=0.338\n",
      "  batch  540: loss=0.335\n",
      "  batch  560: loss=0.393\n",
      "  batch  580: loss=0.360\n",
      "  batch  600: loss=0.332\n",
      "  batch  620: loss=0.367\n",
      "  batch  640: loss=0.404\n",
      "  batch  660: loss=0.378\n",
      "  batch  680: loss=0.347\n",
      "  batch  700: loss=0.353\n",
      "  batch  720: loss=0.367\n",
      "  batch  740: loss=0.420\n",
      "  batch  760: loss=0.383\n",
      "  batch  780: loss=0.346\n",
      "Testing on validation set\n",
      "  acc=0.875\n",
      "Training epoch 3\n",
      "  batch   20: loss=0.308\n",
      "  batch   40: loss=0.319\n",
      "  batch   60: loss=0.339\n",
      "  batch   80: loss=0.352\n",
      "  batch  100: loss=0.310\n",
      "  batch  120: loss=0.378\n",
      "  batch  140: loss=0.340\n",
      "  batch  160: loss=0.356\n",
      "  batch  180: loss=0.317\n",
      "  batch  200: loss=0.335\n",
      "  batch  220: loss=0.394\n",
      "  batch  240: loss=0.360\n",
      "  batch  260: loss=0.335\n",
      "  batch  280: loss=0.350\n",
      "  batch  300: loss=0.324\n",
      "  batch  320: loss=0.321\n",
      "  batch  340: loss=0.391\n",
      "  batch  360: loss=0.420\n",
      "  batch  380: loss=0.338\n",
      "  batch  400: loss=0.357\n",
      "  batch  420: loss=0.338\n",
      "  batch  440: loss=0.345\n",
      "  batch  460: loss=0.334\n",
      "  batch  480: loss=0.330\n",
      "  batch  500: loss=0.356\n",
      "  batch  520: loss=0.361\n",
      "  batch  540: loss=0.366\n",
      "  batch  560: loss=0.311\n",
      "  batch  580: loss=0.332\n",
      "  batch  600: loss=0.313\n",
      "  batch  620: loss=0.363\n",
      "  batch  640: loss=0.345\n",
      "  batch  660: loss=0.362\n",
      "  batch  680: loss=0.338\n",
      "  batch  700: loss=0.362\n",
      "  batch  720: loss=0.348\n",
      "  batch  740: loss=0.351\n",
      "  batch  760: loss=0.367\n",
      "  batch  780: loss=0.341\n",
      "Testing on validation set\n",
      "  acc=0.872\n",
      "Training epoch 4\n",
      "  batch   20: loss=0.330\n",
      "  batch   40: loss=0.313\n",
      "  batch   60: loss=0.378\n",
      "  batch   80: loss=0.362\n",
      "  batch  100: loss=0.325\n",
      "  batch  120: loss=0.349\n",
      "  batch  140: loss=0.323\n",
      "  batch  160: loss=0.316\n",
      "  batch  180: loss=0.330\n",
      "  batch  200: loss=0.300\n",
      "  batch  220: loss=0.343\n",
      "  batch  240: loss=0.368\n",
      "  batch  260: loss=0.339\n",
      "  batch  280: loss=0.304\n",
      "  batch  300: loss=0.353\n",
      "  batch  320: loss=0.358\n",
      "  batch  340: loss=0.327\n",
      "  batch  360: loss=0.326\n",
      "  batch  380: loss=0.368\n",
      "  batch  400: loss=0.330\n",
      "  batch  420: loss=0.298\n",
      "  batch  440: loss=0.324\n",
      "  batch  460: loss=0.350\n",
      "  batch  480: loss=0.328\n",
      "  batch  500: loss=0.328\n",
      "  batch  520: loss=0.325\n",
      "  batch  540: loss=0.346\n",
      "  batch  560: loss=0.340\n",
      "  batch  580: loss=0.280\n",
      "  batch  600: loss=0.322\n",
      "  batch  620: loss=0.339\n",
      "  batch  640: loss=0.361\n",
      "  batch  660: loss=0.339\n",
      "  batch  680: loss=0.310\n",
      "  batch  700: loss=0.329\n",
      "  batch  720: loss=0.326\n",
      "  batch  740: loss=0.334\n",
      "  batch  760: loss=0.341\n",
      "  batch  780: loss=0.357\n",
      "Testing on validation set\n",
      "  acc=0.868\n",
      "Training epoch 5\n",
      "  batch   20: loss=0.311\n",
      "  batch   40: loss=0.332\n",
      "  batch   60: loss=0.314\n",
      "  batch   80: loss=0.314\n",
      "  batch  100: loss=0.336\n",
      "  batch  120: loss=0.327\n",
      "  batch  140: loss=0.314\n",
      "  batch  160: loss=0.319\n",
      "  batch  180: loss=0.338\n",
      "  batch  200: loss=0.321\n",
      "  batch  220: loss=0.366\n",
      "  batch  240: loss=0.357\n",
      "  batch  260: loss=0.342\n",
      "  batch  280: loss=0.346\n",
      "  batch  300: loss=0.322\n",
      "  batch  320: loss=0.295\n",
      "  batch  340: loss=0.350\n",
      "  batch  360: loss=0.318\n",
      "  batch  380: loss=0.315\n",
      "  batch  400: loss=0.311\n",
      "  batch  420: loss=0.342\n",
      "  batch  440: loss=0.321\n",
      "  batch  460: loss=0.288\n",
      "  batch  480: loss=0.319\n",
      "  batch  500: loss=0.315\n",
      "  batch  520: loss=0.280\n",
      "  batch  540: loss=0.367\n",
      "  batch  560: loss=0.325\n",
      "  batch  580: loss=0.286\n",
      "  batch  600: loss=0.344\n",
      "  batch  620: loss=0.284\n",
      "  batch  640: loss=0.332\n",
      "  batch  660: loss=0.369\n",
      "  batch  680: loss=0.321\n",
      "  batch  700: loss=0.325\n",
      "  batch  720: loss=0.350\n",
      "  batch  740: loss=0.291\n",
      "  batch  760: loss=0.333\n",
      "  batch  780: loss=0.320\n",
      "Testing on validation set\n",
      "  acc=0.875\n",
      "Training epoch 6\n",
      "  batch   20: loss=0.348\n",
      "  batch   40: loss=0.349\n",
      "  batch   60: loss=0.367\n",
      "  batch   80: loss=0.311\n",
      "  batch  100: loss=0.288\n",
      "  batch  120: loss=0.301\n",
      "  batch  140: loss=0.325\n",
      "  batch  160: loss=0.382\n",
      "  batch  180: loss=0.323\n",
      "  batch  200: loss=0.312\n",
      "  batch  220: loss=0.302\n",
      "  batch  240: loss=0.298\n",
      "  batch  260: loss=0.286\n",
      "  batch  280: loss=0.290\n",
      "  batch  300: loss=0.335\n",
      "  batch  320: loss=0.374\n",
      "  batch  340: loss=0.305\n",
      "  batch  360: loss=0.299\n",
      "  batch  380: loss=0.349\n",
      "  batch  400: loss=0.334\n",
      "  batch  420: loss=0.322\n",
      "  batch  440: loss=0.343\n",
      "  batch  460: loss=0.287\n",
      "  batch  480: loss=0.343\n",
      "  batch  500: loss=0.307\n",
      "  batch  520: loss=0.316\n",
      "  batch  540: loss=0.281\n",
      "  batch  560: loss=0.322\n",
      "  batch  580: loss=0.321\n",
      "  batch  600: loss=0.293\n",
      "  batch  620: loss=0.287\n",
      "  batch  640: loss=0.330\n",
      "  batch  660: loss=0.309\n",
      "  batch  680: loss=0.310\n",
      "  batch  700: loss=0.339\n",
      "  batch  720: loss=0.358\n",
      "  batch  740: loss=0.278\n",
      "  batch  760: loss=0.311\n",
      "  batch  780: loss=0.328\n",
      "Testing on validation set\n",
      "  acc=0.872\n",
      "Training epoch 7\n",
      "  batch   20: loss=0.317\n",
      "  batch   40: loss=0.297\n",
      "  batch   60: loss=0.292\n",
      "  batch   80: loss=0.290\n",
      "  batch  100: loss=0.314\n",
      "  batch  120: loss=0.336\n",
      "  batch  140: loss=0.303\n",
      "  batch  160: loss=0.308\n",
      "  batch  180: loss=0.326\n",
      "  batch  200: loss=0.269\n",
      "  batch  220: loss=0.296\n",
      "  batch  240: loss=0.289\n",
      "  batch  260: loss=0.280\n",
      "  batch  280: loss=0.275\n",
      "  batch  300: loss=0.323\n",
      "  batch  320: loss=0.274\n",
      "  batch  340: loss=0.282\n",
      "  batch  360: loss=0.321\n",
      "  batch  380: loss=0.336\n",
      "  batch  400: loss=0.339\n",
      "  batch  420: loss=0.331\n",
      "  batch  440: loss=0.325\n",
      "  batch  460: loss=0.305\n",
      "  batch  480: loss=0.321\n",
      "  batch  500: loss=0.320\n",
      "  batch  520: loss=0.326\n",
      "  batch  540: loss=0.302\n",
      "  batch  560: loss=0.322\n",
      "  batch  580: loss=0.323\n",
      "  batch  600: loss=0.318\n",
      "  batch  620: loss=0.304\n",
      "  batch  640: loss=0.325\n",
      "  batch  660: loss=0.353\n",
      "  batch  680: loss=0.283\n",
      "  batch  700: loss=0.296\n",
      "  batch  720: loss=0.354\n",
      "  batch  740: loss=0.300\n",
      "  batch  760: loss=0.315\n",
      "  batch  780: loss=0.284\n",
      "Testing on validation set\n",
      "  acc=0.871\n",
      "Training epoch 8\n",
      "  batch   20: loss=0.345\n",
      "  batch   40: loss=0.281\n",
      "  batch   60: loss=0.312\n",
      "  batch   80: loss=0.264\n",
      "  batch  100: loss=0.287\n",
      "  batch  120: loss=0.309\n",
      "  batch  140: loss=0.275\n",
      "  batch  160: loss=0.287\n",
      "  batch  180: loss=0.325\n",
      "  batch  200: loss=0.323\n",
      "  batch  220: loss=0.301\n",
      "  batch  240: loss=0.268\n",
      "  batch  260: loss=0.317\n",
      "  batch  280: loss=0.304\n",
      "  batch  300: loss=0.284\n",
      "  batch  320: loss=0.285\n",
      "  batch  340: loss=0.350\n",
      "  batch  360: loss=0.339\n",
      "  batch  380: loss=0.323\n",
      "  batch  400: loss=0.306\n",
      "  batch  420: loss=0.313\n",
      "  batch  440: loss=0.339\n",
      "  batch  460: loss=0.303\n",
      "  batch  480: loss=0.295\n",
      "  batch  500: loss=0.310\n",
      "  batch  520: loss=0.288\n",
      "  batch  540: loss=0.339\n",
      "  batch  560: loss=0.295\n",
      "  batch  580: loss=0.342\n",
      "  batch  600: loss=0.293\n",
      "  batch  620: loss=0.285\n",
      "  batch  640: loss=0.280\n",
      "  batch  660: loss=0.294\n",
      "  batch  680: loss=0.330\n",
      "  batch  700: loss=0.286\n",
      "  batch  720: loss=0.319\n",
      "  batch  740: loss=0.292\n",
      "  batch  760: loss=0.319\n",
      "  batch  780: loss=0.316\n",
      "Testing on validation set\n",
      "  acc=0.880\n",
      "Training epoch 9\n",
      "  batch   20: loss=0.325\n",
      "  batch   40: loss=0.319\n",
      "  batch   60: loss=0.308\n",
      "  batch   80: loss=0.259\n",
      "  batch  100: loss=0.307\n",
      "  batch  120: loss=0.283\n",
      "  batch  140: loss=0.288\n",
      "  batch  160: loss=0.286\n",
      "  batch  180: loss=0.322\n",
      "  batch  200: loss=0.311\n",
      "  batch  220: loss=0.274\n",
      "  batch  240: loss=0.309\n",
      "  batch  260: loss=0.330\n",
      "  batch  280: loss=0.306\n",
      "  batch  300: loss=0.283\n",
      "  batch  320: loss=0.255\n",
      "  batch  340: loss=0.308\n",
      "  batch  360: loss=0.325\n",
      "  batch  380: loss=0.290\n",
      "  batch  400: loss=0.304\n",
      "  batch  420: loss=0.335\n",
      "  batch  440: loss=0.315\n",
      "  batch  460: loss=0.303\n",
      "  batch  480: loss=0.303\n",
      "  batch  500: loss=0.286\n",
      "  batch  520: loss=0.298\n",
      "  batch  540: loss=0.292\n",
      "  batch  560: loss=0.303\n",
      "  batch  580: loss=0.327\n",
      "  batch  600: loss=0.297\n",
      "  batch  620: loss=0.295\n",
      "  batch  640: loss=0.278\n",
      "  batch  660: loss=0.318\n",
      "  batch  680: loss=0.319\n",
      "  batch  700: loss=0.317\n",
      "  batch  720: loss=0.324\n",
      "  batch  740: loss=0.336\n",
      "  batch  760: loss=0.331\n",
      "  batch  780: loss=0.333\n",
      "Testing on validation set\n",
      "  acc=0.887\n"
     ]
    }
   ],
   "source": [
    "class Net3(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net3, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "    self.dropout1 = nn.Dropout(p=0.3)\n",
    "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "    self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    self._initialize_weights()\n",
    "  \n",
    "  def _initialize_weights(self):\n",
    "    pass\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.conv1(x))\n",
    "    if self.training:\n",
    "      x = self.dropout1(x)\n",
    "    x = F.max_pool2d(x, 2, stride=2)\n",
    "    x = F.relu(self.conv2(x))\n",
    "    x = F.max_pool2d(x, 2, stride=2)\n",
    "    x = torch.flatten(x, start_dim=1)\n",
    "    x = self.fc3(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "model5 = Net3()\n",
    "model5.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model5.parameters(),\n",
    "    lr=args.lr,\n",
    "    momentum=args.momentum\n",
    ")\n",
    "\n",
    "for e in range(args.num_epochs):\n",
    "  print('Training epoch {}'.format(e))\n",
    "  train(args, model5, criterion, train_loader, optimizer, device)\n",
    "  print('Testing on validation set')\n",
    "  test(args, model5, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6cSM2LLaKd3"
   },
   "source": [
    "The dropout layer increase the accuracy slightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "45ik0nxXUIve"
   },
   "source": [
    "Question 3 - Best network: Increased number of epochs, dropout layer, and increased number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kA070eJiVqL_",
    "outputId": "78523bb0-5b23-40ea-b901-3666eb0b9060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "  batch   20: loss=2.058\n",
      "  batch   40: loss=1.118\n",
      "  batch   60: loss=0.813\n",
      "  batch   80: loss=0.748\n",
      "  batch  100: loss=0.703\n",
      "  batch  120: loss=0.655\n",
      "  batch  140: loss=0.623\n",
      "  batch  160: loss=0.581\n",
      "  batch  180: loss=0.558\n",
      "  batch  200: loss=0.541\n",
      "  batch  220: loss=0.493\n",
      "  batch  240: loss=0.525\n",
      "  batch  260: loss=0.532\n",
      "  batch  280: loss=0.504\n",
      "  batch  300: loss=0.476\n",
      "  batch  320: loss=0.525\n",
      "  batch  340: loss=0.485\n",
      "  batch  360: loss=0.504\n",
      "  batch  380: loss=0.469\n",
      "  batch  400: loss=0.420\n",
      "  batch  420: loss=0.447\n",
      "  batch  440: loss=0.413\n",
      "  batch  460: loss=0.432\n",
      "  batch  480: loss=0.444\n",
      "  batch  500: loss=0.452\n",
      "  batch  520: loss=0.428\n",
      "  batch  540: loss=0.431\n",
      "  batch  560: loss=0.413\n",
      "  batch  580: loss=0.387\n",
      "  batch  600: loss=0.407\n",
      "  batch  620: loss=0.411\n",
      "  batch  640: loss=0.393\n",
      "  batch  660: loss=0.385\n",
      "  batch  680: loss=0.378\n",
      "  batch  700: loss=0.388\n",
      "  batch  720: loss=0.424\n",
      "  batch  740: loss=0.414\n",
      "  batch  760: loss=0.388\n",
      "  batch  780: loss=0.422\n",
      "Testing on validation set\n",
      "  acc=0.852\n",
      "Training epoch 1\n",
      "  batch   20: loss=0.426\n",
      "  batch   40: loss=0.352\n",
      "  batch   60: loss=0.406\n",
      "  batch   80: loss=0.362\n",
      "  batch  100: loss=0.391\n",
      "  batch  120: loss=0.417\n",
      "  batch  140: loss=0.407\n",
      "  batch  160: loss=0.372\n",
      "  batch  180: loss=0.373\n",
      "  batch  200: loss=0.366\n",
      "  batch  220: loss=0.374\n",
      "  batch  240: loss=0.354\n",
      "  batch  260: loss=0.389\n",
      "  batch  280: loss=0.425\n",
      "  batch  300: loss=0.347\n",
      "  batch  320: loss=0.363\n",
      "  batch  340: loss=0.314\n",
      "  batch  360: loss=0.375\n",
      "  batch  380: loss=0.343\n",
      "  batch  400: loss=0.316\n",
      "  batch  420: loss=0.317\n",
      "  batch  440: loss=0.384\n",
      "  batch  460: loss=0.338\n",
      "  batch  480: loss=0.364\n",
      "  batch  500: loss=0.339\n",
      "  batch  520: loss=0.377\n",
      "  batch  540: loss=0.371\n",
      "  batch  560: loss=0.359\n",
      "  batch  580: loss=0.344\n",
      "  batch  600: loss=0.335\n",
      "  batch  620: loss=0.341\n",
      "  batch  640: loss=0.348\n",
      "  batch  660: loss=0.369\n",
      "  batch  680: loss=0.343\n",
      "  batch  700: loss=0.342\n",
      "  batch  720: loss=0.287\n",
      "  batch  740: loss=0.329\n",
      "  batch  760: loss=0.336\n",
      "  batch  780: loss=0.351\n",
      "Testing on validation set\n",
      "  acc=0.879\n",
      "Training epoch 2\n",
      "  batch   20: loss=0.349\n",
      "  batch   40: loss=0.313\n",
      "  batch   60: loss=0.315\n",
      "  batch   80: loss=0.359\n",
      "  batch  100: loss=0.349\n",
      "  batch  120: loss=0.322\n",
      "  batch  140: loss=0.326\n",
      "  batch  160: loss=0.315\n",
      "  batch  180: loss=0.376\n",
      "  batch  200: loss=0.318\n",
      "  batch  220: loss=0.352\n",
      "  batch  240: loss=0.299\n",
      "  batch  260: loss=0.346\n",
      "  batch  280: loss=0.324\n",
      "  batch  300: loss=0.327\n",
      "  batch  320: loss=0.352\n",
      "  batch  340: loss=0.301\n",
      "  batch  360: loss=0.330\n",
      "  batch  380: loss=0.366\n",
      "  batch  400: loss=0.259\n",
      "  batch  420: loss=0.353\n",
      "  batch  440: loss=0.321\n",
      "  batch  460: loss=0.309\n",
      "  batch  480: loss=0.335\n",
      "  batch  500: loss=0.359\n",
      "  batch  520: loss=0.309\n",
      "  batch  540: loss=0.356\n",
      "  batch  560: loss=0.323\n",
      "  batch  580: loss=0.322\n",
      "  batch  600: loss=0.330\n",
      "  batch  620: loss=0.328\n",
      "  batch  640: loss=0.317\n",
      "  batch  660: loss=0.310\n",
      "  batch  680: loss=0.317\n",
      "  batch  700: loss=0.301\n",
      "  batch  720: loss=0.319\n",
      "  batch  740: loss=0.324\n",
      "  batch  760: loss=0.303\n",
      "  batch  780: loss=0.326\n",
      "Testing on validation set\n",
      "  acc=0.880\n"
     ]
    }
   ],
   "source": [
    "class BestNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(BestNet, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "    self.dropout1 = nn.Dropout(p=0.3)\n",
    "    self.conv2 = nn.Conv2d(32, 16, 5)\n",
    "    self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    self._initialize_weights()\n",
    "  \n",
    "  def _initialize_weights(self):\n",
    "    pass\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.conv1(x))\n",
    "    if self.training:\n",
    "      x = self.dropout1(x)\n",
    "    x = F.max_pool2d(x, 2, stride=2)\n",
    "    x = F.relu(self.conv2(x))\n",
    "    x = F.max_pool2d(x, 2, stride=2)\n",
    "    x = torch.flatten(x, start_dim=1)\n",
    "    x = self.fc3(x)\n",
    "    return x\n",
    "\n",
    "best_model = BestNet()\n",
    "best_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    best_model.parameters(),\n",
    "    lr=args.lr,\n",
    "    momentum=args.momentum\n",
    ")\n",
    "\n",
    "for e in range(args.num_epochs):\n",
    "  print('Training epoch {}'.format(e))\n",
    "  train(args, best_model, criterion, train_loader, optimizer, device)\n",
    "  print('Testing on validation set')\n",
    "  test(args, best_model, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZPCnPK7GLEO"
   },
   "source": [
    "Evaluating the best network on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "U7BfvcNkGNrR",
    "outputId": "f41dec87-7cc1-44f7-9444-710117c8116e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  acc=0.880\n"
     ]
    }
   ],
   "source": [
    "  test(args, best_model, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ktEmmfwOHjJT"
   },
   "source": [
    "The best network was able to reach 88% accuracy, which probably can be outperformed with further optimization of hyperparameters and architecture"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fashion_mnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
